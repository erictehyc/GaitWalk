{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This script trains a model which has a ConvLSTM2D layer to encode temporal and spatial information of a video for classification of mood of person depending on gait of the person. \n",
    "\n",
    "Input to model: Sequence of images of person extracted from video (Video strictly contains a person walking towards the camera in frontal view showing his/her full body).\n",
    "\n",
    "Output from model: Probability of a person being in class 1: happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Running Program\n",
    "\n",
    "Move the videos used for training and testing and store in their respective folder named: \"train\" and \"test\" created in base directory\n",
    "**IMPORTANT** video files are named in the format: \"VID_RGB_xxx_y.mp4\" where \"xxx\" is a unique index of the video file and \"y\" is the label of the file where 0: depressed, 1: healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables for training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "SEQ_SIZE = 30\n",
    "MOBILE = False #set to true if input videos are recorded from mobile phone\n",
    "IMG_WIDTH, IMG_HEIGHT = 100, 700 #size of images to be taken in by the ConvLSTM2D layer.\n",
    "EPOCHS = 50\n",
    "FILTER_SIZE = 20 #filter size of the ConvLSTM2D layer\n",
    "LEARNING_RATE = 1e-5\n",
    "KERNEL_SIZE=3 #kernel size of the ConvLSTM2D layer\n",
    "BATCH_SIZE=3\n",
    "LOGS = \"logs\" #set the directory to store logs to be viewed with tensorboard if needed\n",
    "WEIGHTS_DIR = \"weights\" #set the directory where checkpoints of model can be saved to if needed\n",
    "EARLY_STOP = False\n",
    "SAVE_MODEL = True #set to true if want to save model\n",
    "DEBUG = True #set to true when debugging to print out progress during training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpKn7qgxcyMe"
   },
   "source": [
    "### 1.  Perform DeepSORT on input videos.\n",
    "\n",
    "Set up default folder path. This is fixed, do not change the file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnDrNJxOgLeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing session\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from os.path import exists, join, basename\n",
    "from pathlib import Path\n",
    "from fyp_train_gen_img_model import train_model\n",
    "BASE_DIR = os.getcwd()\n",
    "called_dir = BASE_DIR\n",
    "while os.path.basename(BASE_DIR) != \"fyp_team4c\":\n",
    "    path = Path(BASE_DIR)\n",
    "    BASE_DIR = str(path.parent)\n",
    "    if BASE_DIR == '/':\n",
    "        print(\"Please call this script in the fyp_team4c directory\")\n",
    "        break\n",
    "sys.path.append(BASE_DIR)\n",
    "from utils import *\n",
    "\n",
    "TRAINING_DIR = os.path.join(BASE_DIR, 'training')\n",
    "VIDEO_DIRNAMES = [\"train\", \"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhGMc-t_uAO2"
   },
   "source": [
    "Run DeepSort on videos in \"train\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqoRKlRrd7Jt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "Checking if FINAL_VID_DIR exist /home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "Checking if BBOX_DIR exist /home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(TRAINING_DIR, 'output')\n",
    "mobile = True\n",
    "for vid_dirname in VIDEO_DIRNAMES:\n",
    "    videos_dir = os.path.join(TRAINING_DIR, vid_dirname)\n",
    "    vid_l = [f for f in sorted(os.listdir(videos_dir)) if f.endswith(\".mp4\")]\n",
    "    for fn in vid_l:\n",
    "        input_vid_p = os.path.join(videos_dir, fn)\n",
    "        run_deepsort(output_dir, input_vid_p, mobile, called_dir, BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpfaDpeHhVcy"
   },
   "source": [
    "### 2. Extract frames of videos in \"train\" and \"test\" folder and save it to the \"frames\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exg_A3VfhmBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory:  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0010_1.mp4\n",
      "Extracting  train/VID_RGB_0010_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0011_1.mp4\n",
      "Extracting  train/VID_RGB_0011_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0012_1.mp4\n",
      "Extracting  train/VID_RGB_0012_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0013_1.mp4\n",
      "Extracting  train/VID_RGB_0013_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0014_1.mp4\n",
      "Extracting  train/VID_RGB_0014_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0015_1.mp4\n",
      "Extracting  train/VID_RGB_0015_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0016_1.mp4\n",
      "Extracting  train/VID_RGB_0016_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0017_1.mp4\n",
      "Extracting  train/VID_RGB_0017_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_003_1.mp4\n",
      "Extracting  train/VID_RGB_003_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_004_1.mp4\n",
      "Extracting  train/VID_RGB_004_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_006_1.mp4\n",
      "Extracting  train/VID_RGB_006_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_007_1.mp4\n",
      "Extracting  train/VID_RGB_007_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_008_1.mp4\n",
      "Extracting  train/VID_RGB_008_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_009_1.mp4\n",
      "Extracting  train/VID_RGB_009_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_019_0.mp4\n",
      "Extracting  train/VID_RGB_019_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_020_0.mp4\n",
      "Extracting  train/VID_RGB_020_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_021_0.mp4\n",
      "Extracting  train/VID_RGB_021_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_022_0.mp4\n",
      "Extracting  train/VID_RGB_022_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_023_0.mp4\n",
      "Extracting  train/VID_RGB_023_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_024_0.mp4\n",
      "Extracting  train/VID_RGB_024_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_025_0.mp4\n",
      "Extracting  train/VID_RGB_025_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_026_0.mp4\n",
      "Extracting  train/VID_RGB_026_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_027_0.mp4\n",
      "Extracting  train/VID_RGB_027_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_028_0.mp4\n",
      "Extracting  train/VID_RGB_028_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_030_0.mp4\n",
      "Extracting  train/VID_RGB_030_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_032_0.mp4\n",
      "Extracting  train/VID_RGB_032_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_033_0.mp4\n",
      "Extracting  train/VID_RGB_033_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_034_0.mp4\n",
      "Extracting  train/VID_RGB_034_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Creating directory:  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_001_1.mp4\n",
      "Extracting  test/VID_RGB_001_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_002_1.mp4\n",
      "Extracting  test/VID_RGB_002_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_005_1.mp4\n",
      "Extracting  test/VID_RGB_005_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_029_0.mp4\n",
      "Extracting  test/VID_RGB_029_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_031_0.mp4\n",
      "Extracting  test/VID_RGB_031_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_035_0.mp4\n",
      "Extracting  test/VID_RGB_035_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#create the frames folder\n",
    "frames_folder = os.path.join(output_dir, 'frames')\n",
    "\n",
    "        \n",
    "for vid_dirname in VIDEO_DIRNAMES:\n",
    "    vid_dir = os.path.join(TRAINING_DIR, vid_dirname)\n",
    "    vid_frame_dir = os.path.join(frames_folder, vid_dirname)\n",
    "    if not os.path.exists(vid_frame_dir):\n",
    "        os.makedirs(vid_frame_dir)\n",
    "        print(\"Creating directory: \", vid_frame_dir)\n",
    "        \n",
    "    for fn in sorted(os.listdir(vid_dir)):\n",
    "        if fn[-3:] == \"mp4\":\n",
    "            vid_fp = os.path.join(vid_dirname, fn)\n",
    "            print(f'Extracting to {vid_dirname}: ',fn)\n",
    "            extract_frames(vid_fp, fn, vid_frame_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqnxnPYIoi6U"
   },
   "source": [
    "### 3. Extract bounding box of each detected frame located in /frames/train and /frames/test/ and save each extracted box to a folder named with their respective detected ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPf56gTypCet",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting VID_RGB_0010_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0011_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0012_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0013_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0014_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0015_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0016_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0017_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_001_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_002_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_003_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_004_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_005_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_006_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_007_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_008_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_009_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_019_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_020_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_021_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_022_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_023_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_024_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_025_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_026_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_027_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_028_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_029_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_030_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_031_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_032_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_033_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_034_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_035_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n"
     ]
    }
   ],
   "source": [
    "bbox_dir = os.path.join(output_dir, 'bbox_output')\n",
    "\n",
    "for fn in sorted(os.listdir(bbox_dir)):\n",
    "    #fn is in format {filename}_bbox.pkl\n",
    "    vid_fn = f'{fn[:-9]}.mp4'\n",
    "\n",
    "    if fn[-4:] == \".pkl\":\n",
    "        input_frames_folder = \"\"\n",
    "        vid_exists = False\n",
    "    for vid_dirname in VIDEO_DIRNAMES:\n",
    "        vid_dir = os.path.join(TRAINING_DIR, vid_dirname)\n",
    "        vid_fp = os.path.join(vid_dir, vid_fn)\n",
    "        if os.path.exists(vid_fp):\n",
    "#             input_frames_folder = f'{FRAMES_FOLDER}/{vid_dirname}'\n",
    "            input_frames_folder = os.path.join(frames_folder, vid_dirname)\n",
    "            vid_exists=True\n",
    "            break\n",
    "    if not vid_exists:\n",
    "        continue\n",
    "    print(f\"Extracting {fn} from {input_frames_folder}\")\n",
    "    bbox_p = os.path.join(bbox_dir, fn)\n",
    "    extract_bbox(vid_fn[:-4], bbox_p, input_frames_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model with the parameters set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "72 23 HALOOOOOOOOOOOOO\n",
      "Creating model2\n",
      "Compiling model\n",
      "Fitting model with batch size:  3\n",
      "Epoch 1/30\n",
      "Tensor(\"Cast_2:0\", shape=(None, 1), dtype=float32) Tensor(\"sequential_1/dense_5/Sigmoid:0\", shape=(None, 1), dtype=float32)\n",
      "Tensor(\"Cast_2:0\", shape=(None, 1), dtype=float32) Tensor(\"sequential_1/dense_5/Sigmoid:0\", shape=(None, 1), dtype=float32)\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5104 - get_f1: 0.2889Tensor(\"Cast_2:0\", shape=(None, 1), dtype=float32) Tensor(\"sequential_1/dense_5/Sigmoid:0\", shape=(None, 1), dtype=float32)\n",
      "24/24 [==============================] - 30s 1s/step - loss: 0.5104 - get_f1: 0.2889 - val_loss: 0.6995 - val_get_f1: 0.0000e+00\n",
      "Epoch 2/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.5080 - get_f1: 0.3389 - val_loss: 0.7004 - val_get_f1: 0.0000e+00\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.4783 - get_f1: 0.3736 - val_loss: 0.7016 - val_get_f1: 0.0952\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.4664 - get_f1: 0.4361 - val_loss: 0.7039 - val_get_f1: 0.2619\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.4242 - get_f1: 0.5569 - val_loss: 0.7061 - val_get_f1: 0.3095\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.4394 - get_f1: 0.5208 - val_loss: 0.7096 - val_get_f1: 0.3095\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.4256 - get_f1: 0.5361 - val_loss: 0.7118 - val_get_f1: 0.2381\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3805 - get_f1: 0.6222 - val_loss: 0.7082 - val_get_f1: 0.4714\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3596 - get_f1: 0.5958 - val_loss: 0.7054 - val_get_f1: 0.4952\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3566 - get_f1: 0.6625 - val_loss: 0.7068 - val_get_f1: 0.6810\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3611 - get_f1: 0.6278 - val_loss: 0.7195 - val_get_f1: 0.4667\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3414 - get_f1: 0.6431 - val_loss: 0.6840 - val_get_f1: 0.6333\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3513 - get_f1: 0.6431 - val_loss: 0.6817 - val_get_f1: 0.6714\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3342 - get_f1: 0.6264 - val_loss: 0.7675 - val_get_f1: 0.5810\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3152 - get_f1: 0.6958 - val_loss: 0.7096 - val_get_f1: 0.7714\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3060 - get_f1: 0.7139 - val_loss: 0.7661 - val_get_f1: 0.7571\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2881 - get_f1: 0.7278 - val_loss: 0.7745 - val_get_f1: 0.7667\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3009 - get_f1: 0.7125 - val_loss: 0.8208 - val_get_f1: 0.6952\n",
      "Epoch 19/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2834 - get_f1: 0.7056 - val_loss: 0.8488 - val_get_f1: 0.7524\n",
      "Epoch 20/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2933 - get_f1: 0.7708 - val_loss: 0.9268 - val_get_f1: 0.7095\n",
      "Epoch 21/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2931 - get_f1: 0.7486 - val_loss: 0.9724 - val_get_f1: 0.6286\n",
      "Epoch 22/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3014 - get_f1: 0.7333 - val_loss: 0.8610 - val_get_f1: 0.7238\n",
      "Epoch 23/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.3011 - get_f1: 0.6333 - val_loss: 0.7964 - val_get_f1: 0.7762\n",
      "Epoch 24/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2807 - get_f1: 0.7333 - val_loss: 0.9329 - val_get_f1: 0.5810\n",
      "Epoch 25/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2605 - get_f1: 0.7333 - val_loss: 0.7396 - val_get_f1: 0.7238\n",
      "Epoch 26/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2579 - get_f1: 0.7236 - val_loss: 0.9288 - val_get_f1: 0.5905\n",
      "Epoch 27/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2213 - get_f1: 0.7972 - val_loss: 0.8042 - val_get_f1: 0.6524\n",
      "Epoch 28/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2350 - get_f1: 0.7597 - val_loss: 1.1717 - val_get_f1: 0.4476\n",
      "Epoch 29/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2413 - get_f1: 0.7861 - val_loss: 0.9241 - val_get_f1: 0.6810\n",
      "Epoch 30/30\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2486 - get_f1: 0.7708 - val_loss: 1.1608 - val_get_f1: 0.5429\n",
      "[0, 0, 0]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[0, 0, 0]\n",
      "[0, 0]\n",
      "[0, 0, 0]\n",
      "Predicted value in validation set\n",
      "Predicted prob:[0.30173713].\tPredicted val:0\tTrue val: 0\n",
      "Predicted prob:[0.8897834].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.9129242].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.93907565].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.9088871].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.7222617].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.7086361].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.8641966].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.48705694].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.17827821].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.12322628].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.11824761].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.05589778].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.81510544].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.64343965].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.24675216].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.8138464].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.86986625].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.5452169].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.25302175].\tPredicted val:0\tTrue val: 0\n",
      "Predicted prob:[0.9492335].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.7579955].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.47594815].\tPredicted val:0\tTrue val: 0\n",
      "Confusion Matrix\n",
      "[[3 5]\n",
      " [6 9]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Sad       0.33      0.38      0.35         8\n",
      "       Happy       0.64      0.60      0.62        15\n",
      "\n",
      "    accuracy                           0.52        23\n",
      "   macro avg       0.49      0.49      0.49        23\n",
      "weighted avg       0.54      0.52      0.53        23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(IMG_WIDTH, IMG_HEIGHT, 'pickled_seq_images', lr=LEARNING_RATE,batch_size=BATCH_SIZE, epochs=EPOCHS, seq_size=SEQ_SIZE, filter_size=FILTER_SIZE, debug=DEBUG, logs=LOGS, weights_dir=WEIGHTS_DIR,save_model=SAVE_MODEL,early_stop=EARLY_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FYP_Classification_of_Depression_wDeepsort_CNN_LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
