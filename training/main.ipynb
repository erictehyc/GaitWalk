{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This script trains a model which has a ConvLSTM2D layer to encode temporal and spatial information of a video for classification of mood of person depending on gait of the person. \n",
    "\n",
    "Input to model: Sequence of images of person extracted from video (Video strictly contains a person walking towards the camera in frontal view showing his/her full body).\n",
    "\n",
    "Output from model: Probability of a person being in class 1: happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Running Program\n",
    "\n",
    "Move the videos used for training and testing and store in their respective folder named: \"train\" and \"test\" created in base directory\n",
    "**IMPORTANT** video files are named in the format: \"VID_RGB_xxx_y.mp4\" where \"xxx\" is a unique index of the video file and \"y\" is the label of the file where 0: depressed, 1: healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables for training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "SEQ_SIZE = 30\n",
    "MOBILE = False #set to true if input videos are recorded from mobile phone\n",
    "IMG_WIDTH, IMG_HEIGHT = 100, 700 #size of images to be taken in by the ConvLSTM2D layer.\n",
    "EPOCHS = 17\n",
    "FILTER_SIZE = 20 #filter size of the ConvLSTM2D layer\n",
    "LEARNING_RATE = 1e-5\n",
    "KERNEL_SIZE=3 #kernel size of the ConvLSTM2D layer\n",
    "BATCH_SIZE=3\n",
    "LOGS = \"logs\" #set the directory to store logs to be viewed with tensorboard if needed\n",
    "WEIGHTS_DIR = \"weights\" #set the directory where checkpoints of model can be saved to if needed\n",
    "EARLY_STOP = False\n",
    "SAVE_MODEL = True #set to true if want to save model\n",
    "DEBUG = True #set to true when debugging to print out progress during training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpKn7qgxcyMe"
   },
   "source": [
    "### 1.  Perform DeepSORT on input videos.\n",
    "\n",
    "Set up default folder path. This is fixed, do not change the file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnDrNJxOgLeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing session\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from os.path import exists, join, basename\n",
    "from pathlib import Path\n",
    "from fyp_train_gen_img_model import train_model\n",
    "BASE_DIR = os.getcwd()\n",
    "called_dir = BASE_DIR\n",
    "while os.path.basename(BASE_DIR) != \"fyp_team4c\":\n",
    "    path = Path(BASE_DIR)\n",
    "    BASE_DIR = str(path.parent)\n",
    "    if BASE_DIR == '/':\n",
    "        print(\"Please call this script in the fyp_team4c directory\")\n",
    "        break\n",
    "sys.path.append(BASE_DIR)\n",
    "from utils import *\n",
    "\n",
    "TRAINING_DIR = os.path.join(BASE_DIR, 'training')\n",
    "VIDEO_DIRNAMES = [\"train\", \"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhGMc-t_uAO2"
   },
   "source": [
    "Run DeepSort on videos in \"train\" and \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqoRKlRrd7Jt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "Checking if FINAL_VID_DIR exist /home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "Checking if BBOX_DIR exist /home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/train\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n",
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/jiawen/project/fyp_team4c/deep_sort_pytorch\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/final_vid\n",
      "/home/student/jiawen/project/fyp_team4c/training/output/bbox_output\n",
      "/home/student/jiawen/project/fyp_team4c/training/test\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(TRAINING_DIR, 'output')\n",
    "mobile = True\n",
    "for vid_dirname in VIDEO_DIRNAMES:\n",
    "    videos_dir = os.path.join(TRAINING_DIR, vid_dirname)\n",
    "    vid_l = [f for f in sorted(os.listdir(videos_dir)) if f.endswith(\".mp4\")]\n",
    "    for fn in vid_l:\n",
    "        input_vid_p = os.path.join(videos_dir, fn)\n",
    "        run_deepsort(output_dir, input_vid_p, mobile, called_dir, BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpfaDpeHhVcy"
   },
   "source": [
    "### 2. Extract frames of videos in \"train\" and \"test\" folder and save it to the \"frames\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exg_A3VfhmBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory:  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0010_1.mp4\n",
      "Extracting  train/VID_RGB_0010_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0011_1.mp4\n",
      "Extracting  train/VID_RGB_0011_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0012_1.mp4\n",
      "Extracting  train/VID_RGB_0012_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0013_1.mp4\n",
      "Extracting  train/VID_RGB_0013_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0014_1.mp4\n",
      "Extracting  train/VID_RGB_0014_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0015_1.mp4\n",
      "Extracting  train/VID_RGB_0015_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0016_1.mp4\n",
      "Extracting  train/VID_RGB_0016_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_0017_1.mp4\n",
      "Extracting  train/VID_RGB_0017_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_003_1.mp4\n",
      "Extracting  train/VID_RGB_003_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_004_1.mp4\n",
      "Extracting  train/VID_RGB_004_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_006_1.mp4\n",
      "Extracting  train/VID_RGB_006_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_007_1.mp4\n",
      "Extracting  train/VID_RGB_007_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_008_1.mp4\n",
      "Extracting  train/VID_RGB_008_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_009_1.mp4\n",
      "Extracting  train/VID_RGB_009_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_019_0.mp4\n",
      "Extracting  train/VID_RGB_019_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_020_0.mp4\n",
      "Extracting  train/VID_RGB_020_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_021_0.mp4\n",
      "Extracting  train/VID_RGB_021_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_022_0.mp4\n",
      "Extracting  train/VID_RGB_022_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_023_0.mp4\n",
      "Extracting  train/VID_RGB_023_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_024_0.mp4\n",
      "Extracting  train/VID_RGB_024_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_025_0.mp4\n",
      "Extracting  train/VID_RGB_025_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_026_0.mp4\n",
      "Extracting  train/VID_RGB_026_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_027_0.mp4\n",
      "Extracting  train/VID_RGB_027_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_028_0.mp4\n",
      "Extracting  train/VID_RGB_028_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_030_0.mp4\n",
      "Extracting  train/VID_RGB_030_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_032_0.mp4\n",
      "Extracting  train/VID_RGB_032_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_033_0.mp4\n",
      "Extracting  train/VID_RGB_033_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting to train:  VID_RGB_034_0.mp4\n",
      "Extracting  train/VID_RGB_034_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Creating directory:  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_001_1.mp4\n",
      "Extracting  test/VID_RGB_001_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_002_1.mp4\n",
      "Extracting  test/VID_RGB_002_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_005_1.mp4\n",
      "Extracting  test/VID_RGB_005_1.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_029_0.mp4\n",
      "Extracting  test/VID_RGB_029_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_031_0.mp4\n",
      "Extracting  test/VID_RGB_031_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting to test:  VID_RGB_035_0.mp4\n",
      "Extracting  test/VID_RGB_035_0.mp4 to  /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#create the frames folder\n",
    "frames_folder = os.path.join(output_dir, 'frames')\n",
    "\n",
    "        \n",
    "for vid_dirname in VIDEO_DIRNAMES:\n",
    "    vid_dir = os.path.join(TRAINING_DIR, vid_dirname)\n",
    "    vid_frame_dir = os.path.join(frames_folder, vid_dirname)\n",
    "    if not os.path.exists(vid_frame_dir):\n",
    "        os.makedirs(vid_frame_dir)\n",
    "        print(\"Creating directory: \", vid_frame_dir)\n",
    "        \n",
    "    for fn in sorted(os.listdir(vid_dir)):\n",
    "        if fn[-3:] == \"mp4\":\n",
    "            vid_fp = os.path.join(vid_dirname, fn)\n",
    "            print(f'Extracting to {vid_dirname}: ',fn)\n",
    "            extract_frames(vid_fp, fn, vid_frame_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqnxnPYIoi6U"
   },
   "source": [
    "### 3. Extract bounding box of each detected frame located in /frames/train and /frames/test/ and save each extracted box to a folder named with their respective detected ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPf56gTypCet",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting VID_RGB_0010_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0011_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0012_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0013_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0014_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0015_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0016_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_0017_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_001_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_002_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_003_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_004_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_005_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_006_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_007_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_008_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_009_1_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_019_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_020_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_021_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_022_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_023_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_024_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_025_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_026_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_027_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_028_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_029_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_030_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_031_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n",
      "Extracting VID_RGB_032_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_033_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_034_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/train\n",
      "Extracting VID_RGB_035_0_bbox.pkl from /home/student/jiawen/project/fyp_team4c/training/output/frames/test\n"
     ]
    }
   ],
   "source": [
    "bbox_dir = os.path.join(output_dir, 'bbox_output')\n",
    "\n",
    "for fn in sorted(os.listdir(bbox_dir)):\n",
    "    #fn is in format {filename}_bbox.pkl\n",
    "    vid_fn = f'{fn[:-9]}.mp4'\n",
    "\n",
    "    if fn[-4:] == \".pkl\":\n",
    "        input_frames_folder = \"\"\n",
    "        vid_exists = False\n",
    "    for vid_dirname in VIDEO_DIRNAMES:\n",
    "        vid_dir = os.path.join(TRAINING_DIR, vid_dirname)\n",
    "        vid_fp = os.path.join(vid_dir, vid_fn)\n",
    "        if os.path.exists(vid_fp):\n",
    "#             input_frames_folder = f'{FRAMES_FOLDER}/{vid_dirname}'\n",
    "            input_frames_folder = os.path.join(frames_folder, vid_dirname)\n",
    "            vid_exists=True\n",
    "            break\n",
    "    if not vid_exists:\n",
    "        continue\n",
    "    print(f\"Extracting {fn} from {input_frames_folder}\")\n",
    "    bbox_p = os.path.join(bbox_dir, fn)\n",
    "    extract_bbox(vid_fn[:-4], bbox_p, input_frames_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model with the parameters set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "72 23 HALOOOOOOOOOOOOO\n",
      "Creating model2\n",
      "Compiling model\n",
      "Fitting model with batch size:  3\n",
      "Epoch 1/17\n",
      "Tensor(\"Cast_2:0\", shape=(None, 1), dtype=float32) Tensor(\"sequential_5/dense_17/Sigmoid:0\", shape=(None, 1), dtype=float32)\n",
      "Tensor(\"Cast_2:0\", shape=(None, 1), dtype=float32) Tensor(\"sequential_5/dense_17/Sigmoid:0\", shape=(None, 1), dtype=float32)\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3146 - get_f1: 0.5847Tensor(\"Cast_2:0\", shape=(None, 1), dtype=float32) Tensor(\"sequential_5/dense_17/Sigmoid:0\", shape=(None, 1), dtype=float32)\n",
      "24/24 [==============================] - 30s 1s/step - loss: 0.3146 - get_f1: 0.5847 - val_loss: 0.7003 - val_get_f1: 0.0000e+00\n",
      "Epoch 2/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2670 - get_f1: 0.7097 - val_loss: 0.7011 - val_get_f1: 0.0000e+00\n",
      "Epoch 3/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2045 - get_f1: 0.7458 - val_loss: 0.6995 - val_get_f1: 0.0000e+00\n",
      "Epoch 4/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.2020 - get_f1: 0.7903 - val_loss: 0.6963 - val_get_f1: 0.0000e+00\n",
      "Epoch 5/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.1768 - get_f1: 0.8417 - val_loss: 0.6973 - val_get_f1: 0.0000e+00\n",
      "Epoch 6/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.1481 - get_f1: 0.7861 - val_loss: 0.6934 - val_get_f1: 0.1667\n",
      "Epoch 7/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.1296 - get_f1: 0.8000 - val_loss: 0.7110 - val_get_f1: 0.2381\n",
      "Epoch 8/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.1187 - get_f1: 0.8736 - val_loss: 0.6914 - val_get_f1: 0.2381\n",
      "Epoch 9/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0928 - get_f1: 0.9222 - val_loss: 0.6909 - val_get_f1: 0.5429\n",
      "Epoch 10/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0886 - get_f1: 0.9083 - val_loss: 0.6671 - val_get_f1: 0.3714\n",
      "Epoch 11/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0798 - get_f1: 0.9778 - val_loss: 0.6437 - val_get_f1: 0.7048\n",
      "Epoch 12/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0714 - get_f1: 0.9361 - val_loss: 0.6763 - val_get_f1: 0.6619\n",
      "Epoch 13/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0735 - get_f1: 0.9778 - val_loss: 0.6545 - val_get_f1: 0.8238\n",
      "Epoch 14/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0616 - get_f1: 0.9722 - val_loss: 0.5400 - val_get_f1: 0.9000\n",
      "Epoch 15/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0575 - get_f1: 0.9917 - val_loss: 0.6427 - val_get_f1: 0.8095\n",
      "Epoch 16/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0501 - get_f1: 0.9167 - val_loss: 0.7011 - val_get_f1: 0.7381\n",
      "Epoch 17/17\n",
      "24/24 [==============================] - 29s 1s/step - loss: 0.0489 - get_f1: 0.9444 - val_loss: 0.5873 - val_get_f1: 0.7333\n",
      "[0, 0, 0]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[0, 0, 0]\n",
      "[0, 0]\n",
      "[0, 0, 0]\n",
      "Predicted value in validation set\n",
      "Predicted prob:[0.11235976].\tPredicted val:0\tTrue val: 0\n",
      "Predicted prob:[0.7925725].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.86579233].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.90836656].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.9038171].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.7422365].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.77793676].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.63868695].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.60423625].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.82774556].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.8227863].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.8707729].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.6449931].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.70768213].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.45303804].\tPredicted val:0\tTrue val: 1\n",
      "Predicted prob:[0.80218977].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.60789216].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.7093509].\tPredicted val:1\tTrue val: 1\n",
      "Predicted prob:[0.66439474].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.09333453].\tPredicted val:0\tTrue val: 0\n",
      "Predicted prob:[0.77350825].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.937606].\tPredicted val:1\tTrue val: 0\n",
      "Predicted prob:[0.5294581].\tPredicted val:1\tTrue val: 0\n",
      "Confusion Matrix\n",
      "[[ 2  6]\n",
      " [ 1 14]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Sad       0.67      0.25      0.36         8\n",
      "       Happy       0.70      0.93      0.80        15\n",
      "\n",
      "    accuracy                           0.70        23\n",
      "   macro avg       0.68      0.59      0.58        23\n",
      "weighted avg       0.69      0.70      0.65        23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(IMG_WIDTH, IMG_HEIGHT, 'pickled_seq_images', lr=LEARNING_RATE,batch_size=BATCH_SIZE, epochs=EPOCHS, seq_size=SEQ_SIZE, filter_size=FILTER_SIZE, debug=DEBUG, logs=LOGS, weights_dir=WEIGHTS_DIR,save_model=SAVE_MODEL,early_stop=EARLY_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FYP_Classification_of_Depression_wDeepsort_CNN_LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
